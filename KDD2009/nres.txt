
R version 3.2.2 (2015-08-14) -- "Fire Safety"
Copyright (C) 2015 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> 
> # Plot effect of training y-noise on fit quality
> 
> source('Lfns.R')
> source('Afns.R')
Loading required package: ggplot2
Loading required package: grid
Loading required package: gridExtra
Loading required package: reshape2
Loading required package: ROCR
Loading required package: gplots

Attaching package: ‘gplots’

The following object is masked from ‘package:stats’:

    lowess

Loading required package: plyr
Loading required package: stringr
Loading required package: survival
Loading required package: lattice
Loading required package: splines
Loaded gbm 2.1.1
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.

Attaching package: ‘dplyr’

The following object is masked from ‘package:randomForest’:

    combine

The following objects are masked from ‘package:data.table’:

    between, last

The following objects are masked from ‘package:plyr’:

    arrange, count, desc, failwith, id, mutate, rename, summarise,
    summarize

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union


Attaching package: ‘caret’

The following object is masked from ‘package:survival’:

    cluster

> 
> debug = FALSE
> 
> # load the data as in the book
> # change this path to match your directory structure
> dir = './'
> if(debug) {
+    dir = '~/Documents/work/PracticalDataScienceWithR/zmPDSwR/KDD2009/'
+ }
> 
> d = read.table(paste(dir,'orange_small_train.data.gz',sep=''),
+                header=T,sep='\t',na.strings=c('NA',''), 
+                stringsAsFactors=FALSE)
> churn = read.table(paste(dir,'orange_small_train_churn.labels.txt',sep=''),
+                    header=F,sep='\t')
> d$churn = churn$V1
> appetency = read.table(paste(dir,'orange_small_train_appetency.labels.txt',sep=''),
+                        header=F,sep='\t')
> d$appetency = appetency$V1
> upselling = read.table(paste(dir,'orange_small_train_upselling.labels.txt',sep=''),
+                        header=F,sep='\t')
> d$upselling = upselling$V1
> set.seed(729375)
> d$rgroup = runif(dim(d)[[1]])
> dTrainM = subset(d,rgroup<=0.7)  # set for building models
> dTrainC = subset(d,(rgroup>0.7) & (rgroup<=0.9)) # set for impact coding
> dTest = subset(d,rgroup>0.9) # set for evaluation
> rm(list=c('d','churn','appetency','upselling','dir'))
> outcomes = c('churn','appetency','upselling')
> vars = setdiff(colnames(dTrainM),
+                c(outcomes,'rgroup'))
> yName = 'churn'
> yTarget = 1
> 
> set.seed(239525)
> nCoreEstimate <-  parallel::detectCores()
> print(paste('core estimate',nCoreEstimate))
[1] "core estimate 36"
> parallelCluster = parallel::makeCluster(nCoreEstimate)
> 
> # build treatments on just the coding data
> treatmentsC = designTreatmentsC(dTrainC,
+                                 vars,yName,yTarget,
+                                 smFactor=2.0, 
+                                 parallelCluster=parallelCluster)
[1] "desigining treatments Mon Aug 31 18:16:00 2015"
[1] "scoring treatments Mon Aug 31 18:16:01 2015"
[1] "WARNING skipped vars: Var8, Var15, Var20, Var31, Var32, Var39, Var42, Var48, Var52, Var55, Var79, Var141, Var167, Var169, Var175, Var185, Var209, Var230"
[1] "have treatment plan Mon Aug 31 18:16:08 2015"
> 
> 
> # prepare data
> treatedTrainM = prepare(treatmentsC,
+                         dTrainM,
+                         pruneSig=0.05)
> varSet = setdiff(colnames(treatedTrainM),yName)
> treatedTrainM[[yName]] = treatedTrainM[[yName]]==yTarget
> print(summary(treatedTrainM[[yName]]))
   Mode   FALSE    TRUE    NA's 
logical   32449    2572       0 
> 
> treatedTest = prepare(treatmentsC,
+                       dTest,
+                       pruneSig=0.05)
> treatedTest[[yName]] = treatedTest[[yName]]==yTarget
> print(summary(treatedTest[[yName]]))
   Mode   FALSE    TRUE    NA's 
logical    4619     353       0 
> 
> 
> chosenVars <- names(treatmentsC$sig)[treatmentsC$sig<0.05]
> 
> if(debug) {
+   chosenVars <- chosenVars[1:5]
+   treatedTrainM <- treatedTrainM[sample.int(nrow(treatedTrainM),200),]
+   treatedTest <- treatedTest[sample.int(nrow(treatedTest),200),]
+ }
> 
> 
> 
> 
> # get performance on test
> scoreFList <- list(test=treatedTest,train=treatedTrainM)
> # build work list
> workList <- list()
> for(rep in 1:2) {
+   for(noiseLevel in c(0,0.05,0.1,0.15,0.2,0.25,0.3,0.35)) {
+     # noise up some fraction of positions
+     nnoise <- floor(noiseLevel*nrow(treatedTrainM))
+     noiseIndices <- numeric(0)
+     noiseValues <- logical(0)
+     if(nnoise>0) {
+       noiseIndices <- sample.int(nrow(treatedTrainM),nnoise)
+       noiseValues <- rbinom(nnoise,1,prob=0.5)>=0.5
+     }
+     for(mn in names(allFitters)) {
+       workList[[1+length(workList)]] <- list(modelTitle=mn,
+                                              noiseLevel=noiseLevel,
+                                              rep=rep,
+                                              noiseIndices=noiseIndices,
+                                              noiseValues=noiseValues)
+     }
+   }
+ }
> 
> mkWorkerN1 <- function(allFitters,yName,chosenVars,treatedTrainX,scoreFList) {
+   force(allFitters)
+   force(yName)
+   force(chosenVars)
+   force(treatedTrainX)
+   force(scoreFList)
+   function(workUnit) {
+     source('Lfns.R')
+     source('Afns.R')
+     modelTitle <- workUnit$modelTitle
+     noiseLevel <- workUnit$noiseLevel
+     rep  <- workUnit$rep
+     noiseIndices  <- workUnit$noiseIndices
+     noiseValues  <- workUnit$noiseValues
+     fitter <- allFitters[[modelTitle]]
+     if(length(noiseIndices)>0) {
+       treatedTrainX[[yName]][noiseIndices] <- noiseValues
+     }
+     scoreFList$ntrain <- treatedTrainX
+     perfScores <- fitter(yName,chosenVars,treatedTrainX,scoreFList,
+                          bootScore=FALSE,parallelCluster=c())
+     cbind(data.frame(model=modelTitle,
+                      noiseLevel=noiseLevel,
+                      rep=rep,
+                      stringsAsFactors=FALSE),
+           data.frame(as.list(unlist(perfScores))))
+   } 
+ }
> w1 <- mkWorkerN1(allFitters,yName,chosenVars,
+                  treatedTrainM,scoreFList)
> if(!debug) {
+   resList <- parallel::parLapply(parallelCluster,workList,w1)
+ } else {
+   # run directly (without parLapply or lapply) to make tracing in easier
+   resList <- vector(mode='list',length=length(workList))
+   for(i in seq_len(length(workList))) {
+     print(paste('start',i,date()))
+     wi <- workList[[i]]
+     resList[[i]] <- w1(wi)
+     print(paste('done',i,date()))
+   }
+ }
> if(length(resList)!=length(workList)) {
+   stop("not all results came back from parLapply")
+ }
> 
> 
> 
> 
> 
> # shutdown, clean up
> if(!is.null(parallelCluster)) {
+   parallel::stopCluster(parallelCluster)
+   parallelCluster = NULL
+ }
> 
> rf = data.frame(data.table::rbindlist(resList))
> saveRDS(rf,file='nResFrame.RData')
> 
> 
> proc.time()
    user   system  elapsed 
  15.442    1.650 6198.787 
