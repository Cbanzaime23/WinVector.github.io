
[KDD2009 example](http://www.sigkdd.org/kdd-cup-2009-customer-relationship-prediction).  Winners had hold-out AUC of 0.7611 on churn.   See [here](https://github.com/WinVector/zmPDSwR/tree/master/KDD2009) for more details.

```{r kddexlibs, tidy=FALSE}
#load some libraries
# http://www.win-vector.com/blog/2014/08/vtreat-designing-a-package-for-variable-treatment/
library('vtreat')
# devtools::install_github("WinVector/WVPlots")
library('WVPlots')

library('parallel')
#library('class')
library('ggplot2')
library('glmnet')

source("xgboost.R")

# load the data as in the book
# change this path to match your directory structure
dir = '../../PracticalDataScienceWithR/zmPDSwR/KDD2009/' 

d = read.table(paste(dir,'orange_small_train.data.gz',sep=''),
                header=T,sep='\t',na.strings=c('NA',''), 
               stringsAsFactors=FALSE)
churn = read.table(paste(dir,'orange_small_train_churn.labels.txt',sep=''),
                    header=F,sep='\t')
d$churn = churn$V1
appetency = read.table(paste(dir,'orange_small_train_appetency.labels.txt',sep=''),
                        header=F,sep='\t')
d$appetency = appetency$V1
upselling = read.table(paste(dir,'orange_small_train_upselling.labels.txt',sep=''),
                        header=F,sep='\t')
d$upselling = upselling$V1
set.seed(729375)
d$rgroup = runif(dim(d)[[1]])
dTrain = subset(d,rgroup<=0.9)  # set for building models
dTest = subset(d,rgroup>0.9) # set for evaluation
debug = FALSE
if(debug) {
  dTrain <- dTrain[sample.int(nrow(dTrainM),100),]
  dTest <- dTest[sample.int(nrow(dTest),100),]
}
rm(list=c('d','churn','appetency','upselling','dir'))
outcomes = c('churn','appetency','upselling')
nonvars <- c(outcomes,'rgroup')
vars = setdiff(colnames(dTrain),
                nonvars)
yName = 'churn'
yTarget = 1
```



```{r kdddesign, tidy=FALSE}
# build data treatments

set.seed(239525)

cl <- c()
if(!debug) {
  ncore <- parallel::detectCores()
  cl <- parallel::makeCluster(ncore)
}

# build treatments 
kddSig = 0.05

trainPlan = mkCrossFrameCExperiment(dTrain,
    vars,yName,yTarget,
    smFactor=2.0, rareCount = 2, rareSig = 0.5,
    parallelCluster=cl)
treatmentsC = trainPlan$treatments
treatedTrainM = trainPlan$crossFrame

selvars = treatmentsC$scoreFrame$varName[treatmentsC$scoreFrame$sig<kddSig]

print(treatmentsC$scoreFrame[,c('varName','sig')])

treatedTrainM[[yName]] = treatedTrainM[[yName]]==yTarget

treatedTest = prepare(treatmentsC,
                      dTest,
                      pruneSig=kddSig, 
                      parallelCluster=cl)
treatedTest[[yName]] = treatedTest[[yName]]==yTarget


if(!is.null(cl)) {
    parallel::stopCluster(cl)
    cl = NULL
}
```





```{r kddmodels, tidy=FALSE}
# Run other models (with proper coding/training separation).
#
# This gets us back to AUC 0.72

#print(selvars)

# prepare plotting frames
treatedTrainP = treatedTrainM[, yName, drop=FALSE]
treatedTestP = treatedTest[, yName, drop=FALSE]


formulaS = paste(yName,paste(selvars,collapse=' + '),sep=' ~ ')
for(mname in c('glmPred','xgboost')) {
  print("*****************************")
  print(date())
  print(paste(mname,length(selvars)))
  if(mname=='xgboost') {
    m <- mkXGBoostModel(treatedTrainM, selvars, yName)
    treatedTestP[[mname]] = m(treatedTest)
  } else {
    modelglms = cv.glmnet(x = as.matrix(treatedTrainM[,selvars,drop=FALSE]),
                          y = treatedTrainM[[yName]],
                          alpha=0.5,
                          family='binomial')
    #print(summary(modelglms))
    treatedTestP[[mname]] = as.numeric(predict(modelglms,
                                               newx=as.matrix(treatedTest[,selvars,drop=FALSE]),
                                               type='response'))
  }
  
  t2 = paste(mname,'test data')
  print(DoubleDensityPlot(treatedTestP, mname, yName, 
                          title=t2))
  print(ROCPlot(treatedTestP, mname, yName, yTarget,
                title=t2))
  print(date())
  print("*****************************")
}

```
