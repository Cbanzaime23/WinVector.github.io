
[KDD2009 example](http://www.sigkdd.org/kdd-cup-2009-customer-relationship-prediction).  Winners had hold-out AUC of 0.7611 on churn.   See [here](https://github.com/WinVector/zmPDSwR/tree/master/KDD2009) for more details.

```{r kddexlibs, tidy=FALSE}
#load some libraries
# http://www.win-vector.com/blog/2014/08/vtreat-designing-a-package-for-variable-treatment/
library('vtreat')
# devtools::install_github("WinVector/WVPlots")
library('WVPlots')

library('parallel')
library('gbm')
library('class')
library('ggplot2')


# load the data as in the book
# change this path to match your directory structure
dir = '~/Documents/work/PracticalDataScienceWithR/zmPDSwR/KDD2009/' 

d = read.table(paste(dir,'orange_small_train.data.gz',sep=''),
                header=T,sep='\t',na.strings=c('NA',''), 
               stringsAsFactors=FALSE)
churn = read.table(paste(dir,'orange_small_train_churn.labels.txt',sep=''),
                    header=F,sep='\t')
d$churn = churn$V1
appetency = read.table(paste(dir,'orange_small_train_appetency.labels.txt',sep=''),
                        header=F,sep='\t')
d$appetency = appetency$V1
upselling = read.table(paste(dir,'orange_small_train_upselling.labels.txt',sep=''),
                        header=F,sep='\t')
d$upselling = upselling$V1
set.seed(729375)
d$rgroup = runif(dim(d)[[1]])
dTrainM = subset(d,rgroup<=0.5)  # set for building models
dTrainC = subset(d,(rgroup>0.5) & (rgroup<=0.9)) # set for impact coding
dTest = subset(d,rgroup>0.9) # set for evaluation
debug = FALSE
if(debug) {
  dTrainM <- dTrainM[sample.int(nrow(dTrainM),100),]
  dTrainC <- dTrainC[sample.int(nrow(dTrainC),100),]
  dTest <- dTest[sample.int(nrow(dTest),100),]
}
rm(list=c('d','churn','appetency','upselling','dir'))
outcomes = c('churn','appetency','upselling')
nonvars <- c(outcomes,'rgroup')
vars = setdiff(colnames(dTrainM),
                nonvars)
yName = 'churn'
yTarget = 1
```



```{r kdddesign, tidy=FALSE}
# build data treatments

set.seed(239525)

cl <- c()
if(!debug) {
  ncore <- parallel::detectCores()
  cl <- parallel::makeCluster(ncore)
}

# build treatments on just the coding data
treatmentsC = designTreatmentsC(dTrainC,
    vars,yName,yTarget,
    smFactor=2.0, 
    parallelCluster=cl,
    verbose=TRUE)

kddSig = 0.05

addResidualVars = FALSE
addOtherTargets = FALSE

treatedTrainM = prepare(treatmentsC,
                        dTrainM,
                        pruneSig=kddSig, 
                        parallelCluster=cl)
selvars = setdiff(colnames(treatedTrainM),nonvars)
treatedTrainM[[yName]] = treatedTrainM[[yName]]==yTarget

treatedTest = prepare(treatmentsC,
                      dTest,
                      pruneSig=kddSig, 
                      parallelCluster=cl)
treatedTest[[yName]] = treatedTest[[yName]]==yTarget

if(addOtherTargets) {
  addResidualVars = FALSE
  for(yV in c('appetency','upselling')) {
    treatmentsCO = designTreatmentsC(dTrainC,
                                     vars,yV,yTarget,
                                     smFactor=2.0, 
                                     parallelCluster=cl)
    treatedTrainMO = prepare(treatmentsCO,
                             dTrainM,
                             pruneSig=kddSig, 
                             parallelCluster=cl)
    treatedTrainMO[[yV]] = treatedTrainMO[[yV]]==yTarget
    treatedTestO = prepare(treatmentsCO,
                             dTest,
                             pruneSig=kddSig, 
                             parallelCluster=cl)
    treatedTestO[[yV]] = treatedTestO[[yV]]==yTarget
    selvarsO = setdiff(colnames(treatedTrainMO),nonvars)
    formulaSO = paste(yV,paste(selvarsO,collapse=' + '),sep=' ~ ')
    modelGBMsO = gbm(as.formula(formulaSO),
                     data=treatedTrainMO,
                     distribution='bernoulli',
                     n.trees=1000,
                     interaction.depth=3,
                     keep.data=FALSE,
                     cv.folds=5)
    nTrees = gbm.perf(modelGBMsO,plot.it=FALSE)
    vname <- paste('pred',yV,sep='_')
    treatedTrainM[[vname]] = predict(modelGBMsO,newdata=treatedTrainMO,
                                     type='response',
                                     n.trees=nTrees) 
    treatedTest[[vname]] = predict(modelGBMsO,newdata=treatedTestO,
                                    type='response',
                                    n.trees=nTrees)
  }
  selvars = setdiff(colnames(treatedTrainM),nonvars)
}

if(addResidualVars) {
  # see if we can learn new encodings from residuals of a mdoel
  if('devCorrection' %in% colnames(dTrainC)) {
    stop("devCorrection already used")
  }
  # get the calibration frame ready
  treatedTrainC = prepare(treatmentsC,
                          dTrainC,
                          pruneSig=kddSig, 
                          parallelCluster=cl)
  treatedTrainC[[yName]] = treatedTrainC[[yName]]==yTarget
  # build an initial model
  formulaS = paste(yName,paste(selvars,collapse=' + '),sep=' ~ ')
  modelGBMs = gbm(as.formula(formulaS),
                  data=treatedTrainC,
                  distribution='bernoulli',
                  n.trees=500,
                  interaction.depth=3,
                  keep.data=FALSE,
                  cv.folds=5)
  nTrees = gbm.perf(modelGBMs,plot.it = FALSE)
  mpred = predict(modelGBMs,newdata=treatedTrainC,type='response',
                  n.trees=nTrees)
  # build new variables 
  epsilon = 1.0e-5
  # collar prediction to make log well behaved
  mpred = pmin(pmax(mpred,epsilon),1-epsilon)
  # deviance correcton, want more positive on true examples and more negative on false
  # -gradient of the deviance per-row, shifted towards zero
  # devCorrection = ifelse(treatedTrainC[[yName]],2/mpred-2,-2/(1-mpred)+2)
  # or, assuming p = sigmoid(link) we can take the derivative wrt link and get (1-p)-(1-y) = y-p
  devCorrection = ifelse(treatedTrainC[[yName]],1.0,0.0) - mpred
  pf <- data.frame(pred=mpred,y=treatedTrainC[[yName]],devCorrection=devCorrection)
  pf$match <- pf$y==(pf$pred>0.5)
  ggplot(data=pf,aes(x=devCorrection,color=match)) + geom_density()
  dTrainC$devCorrection <- devCorrection
  catVars <- selvars[grep('_cat',selvars,fixed=TRUE)]
  catPosns <- treatmentsC$scoreFrame$varName %in% catVars
  catOrig <- unique(treatmentsC$scoreFrame[catPosns,'origName',drop=TRUE])
  treatmentsN = designTreatmentsN(dTrainC,
                                  catOrig,'devCorrection',
                                  minFraction=1.0,
                                  smFactor=2.0, 
                                  parallelCluster=cl)
  newVars <- treatmentsN$scoreFrame[treatmentsN$scoreFrame$sig<=kddSig,
                                    'varName',drop=TRUE]
  if(length(intersect(newVars,selvars))>0) {
    stop("variable name re-used")
  }
  if(length(intersect(c(newVars,selvars),c(yName,'devCorrection')))>0) {
    stop("variable name clash")
  }
  # disjoint outcome name, so not in the original frames to be compied over
  treatedTrainN = prepare(treatmentsN,
                          dTrainM,
                          pruneSig=c(), 
                          varRestriction=newVars,
                          parallelCluster=cl)
  treatedTrainM = cbind(treatedTrainM,treatedTrainN)
  treatedTestN = prepare(treatmentsN,
                         dTest,
                         pruneSig=c(), 
                         varRestriction=newVars,
                         parallelCluster=cl)
  treatedTest = cbind(treatedTest,treatedTestN)
  selvars <- c(selvars,newVars)
}

if(!is.null(cl)) {
    parallel::stopCluster(cl)
    cl = NULL
}
```





```{r kddmodels, tidy=FALSE}
# Run other models (with proper coding/training separation).
#
# This gets us back to AUC 0.72

print(selvars)

# prepare plotting frames
treatedTrainP = treatedTrainM[, yName, drop=FALSE]
treatedTestP = treatedTest[, yName, drop=FALSE]


formulaS = paste(yName,paste(selvars,collapse=' + '),sep=' ~ ')
for(mname in c('gbmPred','glmPred')) {
  print("*****************************")
  print(date())
  print(paste(mname,length(selvars)))
  if(mname=='gbmPred') {
    modelGBMs = gbm(as.formula(formulaS),
                    data=treatedTrainM,
                    distribution='bernoulli',
                    n.trees=1000,
                    interaction.depth=3,
                    keep.data=FALSE,
                    cv.folds=5)
    #print(modelGBMs)
    #print(summary(modelGBMs))
    nTrees = gbm.perf(modelGBMs)
    treatedTrainP[[mname]] = predict(modelGBMs,newdata=treatedTrainM,type='response',
                                     n.trees=nTrees) 
    treatedTestP[[mname]] = predict(modelGBMs,newdata=treatedTest,type='response',
                                    n.trees=nTrees)
  } else {
    modelglms = glm(as.formula(formulaS),
                    data=treatedTrainM,
                    family=binomial(link='logit')
    )
    #print(summary(modelglms))
    treatedTrainP[[mname]] = predict(modelglms,newdata=treatedTrainM,type='response')
    treatedTestP[[mname]] = predict(modelglms,newdata=treatedTest,type='response')
  }
  
  t1 = paste(mname,'trainingM data')
  print(DoubleDensityPlot(treatedTrainP, mname, yName, 
                          title=t1))
  print(ROCPlot(treatedTrainP, mname, yName, 
                title=t1))
  
  t2 = paste(mname,'test data')
  print(DoubleDensityPlot(treatedTestP, mname, yName, 
                          title=t2))
  print(ROCPlot(treatedTestP, mname, yName, 
                title=t2))
  print(date())
  print("*****************************")
}

```
