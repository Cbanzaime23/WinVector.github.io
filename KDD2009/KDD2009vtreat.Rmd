


```{r kddexlibs, tidy=FALSE}
#load some libraries
# http://www.win-vector.com/blog/2014/08/vtreat-designing-a-package-for-variable-treatment/
# devtools::install_github("WinVector/vtreat")
library('vtreat')
# devtools::install_github("WinVector/vtreat")
library('WVPlots')

library('parallel')
library('gbm')
library('class')


# load the data as in the book
# change this path to match your directory structure
dir = '~/Documents/work/PracticalDataScienceWithR/zmPDSwR/KDD2009/' 

d = read.table(paste(dir,'orange_small_train.data.gz',sep=''),
                header=T,sep='\t',na.strings=c('NA',''), 
               stringsAsFactors=FALSE)
churn = read.table(paste(dir,'orange_small_train_churn.labels.txt',sep=''),
                    header=F,sep='\t')
d$churn = churn$V1
appetency = read.table(paste(dir,'orange_small_train_appetency.labels.txt',sep=''),
                        header=F,sep='\t')
d$appetency = appetency$V1
upselling = read.table(paste(dir,'orange_small_train_upselling.labels.txt',sep=''),
                        header=F,sep='\t')
d$upselling = upselling$V1
set.seed(729375)
d$rgroup = runif(dim(d)[[1]])
dTrainM = subset(d,rgroup<=0.5)  # set for building models
dTrainC = subset(d,(rgroup>0.5) & (rgroup<=0.9)) # set for impact coding
dTest = subset(d,rgroup>0.9) # set for evaluation
rm(list=c('d','churn','appetency','upselling','dir'))
outcomes = c('churn','appetency','upselling')
vars = setdiff(colnames(dTrainM),
                c(outcomes,'rgroup'))
yName = 'churn'
yTarget = 1
```



```{r kdddesign, tidy=FALSE}
# build data treatments

set.seed(239525)

cl = parallel::makeCluster(4)


# build treatments on just the coding data
treatmentsC = designTreatmentsC(dTrainC,
    vars,yName,yTarget,
    smFactor=2.0, 
    parallelCluster=cl,
    scoreVars=TRUE)

if(!is.null(cl)) {
    parallel::stopCluster(cl)
    cl = NULL
}
```


```{r kddknnnew, tidy=FALSE}
kddPrune=0.999
nK = 200
knnTrain = prepare(treatmentsC,
                   dTrainM,
                   scale=TRUE,
                   pruneLevel=kddPrune)
selvars = setdiff(colnames(knnTrain),yName)
knnCl = knnTrain[[yName]]==yTarget

knnTest = prepare(treatmentsC,
                  dTest,
                  scale=TRUE,
                  pruneLevel=kddPrune)

knnPred = function(df) {
    knnDecision = knn(knnTrain[,selvars],df[,selvars],knnCl,k=nK,prob=T)
    ifelse(knnDecision==TRUE,
       attributes(knnDecision)$prob,
       1-(attributes(knnDecision)$prob))
}
knnTest$predKNN = knnPred(knnTest)
knnTest[[yName]] = knnTest[[yName]]==yTarget
ti = 'KNN code on C nn on M'
print(DoubleDensityPlot(knnTest, 'predKNN', yName, 
                               title=ti))
print(ROCPlot(knnTest, 'predKNN', yName, 
                     title=ti))

```




```{r kddmodels, tidy=FALSE}
# Run other models (with proper coding/training separation).
#
# This gets us back to AUC 0.72

kddPrune = 0.999

treatedTrainM = prepare(treatmentsC,
                   dTrainM,
                   pruneLevel=kddPrune)
selvars = setdiff(colnames(knnTrain),yName)
treatedTrainM[[yName]] = treatedTrainM[[yName]]==yTarget

treatedTest = prepare(treatmentsC,
                  dTest,
                  pruneLevel=kddPrune)
treatedTest[[yName]] = treatedTest[[yName]]==yTarget

# prepare plotting frames
treatedTrainP = treatedTrainM[, yName, drop=FALSE]
treatedTestP = treatedTest[, yName, drop=FALSE]


formulaS = paste(yName,paste(selvars,collapse=' + '),sep=' ~ ')
for(mname in c('gbmPred','glmPred')) {
  print("*****************************")
  print(date())
  print(paste(mname,length(selvars)))
  if(mname=='gbmPred') {
    modelGBMs = gbm(as.formula(formulaS),
                    data=treatedTrainM,
                    distribution='bernoulli',
                    n.trees=500,
                    interaction.depth=3,
                    keep.data=FALSE,
                    cv.folds=5)
    print(modelGBMs)
    print(summary(modelGBMs))
    nTrees = gbm.perf(modelGBMs)
    treatedTrainP[[mname]] = predict(modelGBMs,newdata=treatedTrainM,type='response',
                                     n.trees=nTrees) 
    treatedTestP[[mname]] = predict(modelGBMs,newdata=treatedTest,type='response',
                                    n.trees=nTrees)
  } else {
    modelglms = glm(as.formula(formulaS),
                    data=treatedTrainM,
                    family=binomial(link='logit')
    )
    print(summary(modelglms))
    treatedTrainP[[mname]] = predict(modelglms,newdata=treatedTrainM,type='response')
    treatedTestP[[mname]] = predict(modelglms,newdata=treatedTest,type='response')
  }
  
  t1 = paste(mname,'trainingM data')
  print(DoubleDensityPlot(treatedTrainP, mname, yName, 
                          title=t1))
  print(ROCPlot(treatedTrainP, mname, yName, 
                title=t1))
  
  t2 = paste(mname,'test data')
  print(DoubleDensityPlot(treatedTestP, mname, yName, 
                          title=t2))
  print(ROCPlot(treatedTestP, mname, yName, 
                title=t2))
  print(date())
  print("*****************************")
}

```
