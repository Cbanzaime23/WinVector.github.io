---
title: "KDD2009xgboost"
author: "Win-Vector LLC"
date: "July 18, 2016"
output: html_document
---

```{r setup}
knitr::opts_chunk$set(echo = TRUE)
library('vtreat')
library('WVPlots') # see: https://github.com/WinVector/WVPlots
library('xgboost')
```

```{r init}
ncore <- parallel::detectCores()
cl <- parallel::makeCluster(ncore)
```

```{r loaddata}
# see: https://github.com/WinVector/PreparingDataWorkshop/tree/master/KDD2009
d = read.table('orange_small_train.data.gz',
               header=T,sep='\t',na.strings=c('NA',''), 
               strip.white = TRUE,
               stringsAsFactors=FALSE)
churn = read.table('orange_small_train_churn.labels.txt',
                   header=F,sep='\t',
                   strip.white = TRUE,
                   stringsAsFactors = FALSE)
d$churn = churn$V1
set.seed(729375)
rgroup = runif(dim(d)[[1]])
dTrain = d[rgroup<=0.9,]  # set for building models
dTest = d[rgroup>0.9,] # set for evaluation
rm(list=c('d','churn'))
outcomes = c('churn','appetency','upselling')
nonvars <- c(outcomes,'rgroup')
vars = setdiff(colnames(dTrain),
                nonvars)
yName = 'churn'
yTarget = 1
```

```{r preparedata}
# build data treatments
set.seed(239525)

# build treatments 
trainPlan = mkCrossFrameCExperiment(dTrain,
    vars,yName,yTarget,
    smFactor=2.0, 
    parallelCluster=cl)
print(trainPlan$method)
treatmentsC = trainPlan$treatments
treatedTrainM = trainPlan$crossFrame

#kddSig = 1/nrow(treatmentsC$scoreFrame)
selvars <- setdiff(colnames(treatedTrainM),outcomes)
treatedTrainM[[yName]] = treatedTrainM[[yName]]==yTarget

treatedTest = prepare(treatmentsC,
                      dTest,
                      varRestriction=selvars,
                      pruneSig=NULL, 
                      parallelCluster=cl)
treatedTest[[yName]] = treatedTest[[yName]]==yTarget
```


```{r fit1}
mname <- 'predxgboost'

# simple default, production model would require hyperparameter search
goodvars <- treatmentsC$scoreFrame$varName[treatmentsC$scoreFrame$sig<1/nrow(treatmentsC$scoreFrame)]
formulaS = paste(yName,paste(goodvars,collapse=' + '),sep=' ~ ')

for(ntrees in c(50,100,200)) {
  modelxg = xgboost(data=xgb.DMatrix(as.matrix(treatedTrainM[,goodvars,drop=FALSE]),
                                     label=treatedTrainM[[yName]]),
                    objective='binary:logistic', 
                    nrounds=ntrees,
                    nthread=ncore)
  # prepare plotting frames
  treatedTrainP = treatedTrainM[, yName, drop=FALSE]
  treatedTestP = treatedTest[, yName, drop=FALSE]
  treatedTrainP[[mname]] = as.numeric(predict(modelxg,
                                              as.matrix(treatedTrainM[,goodvars,drop=FALSE])))
  treatedTestP[[mname]] = as.numeric(predict(modelxg,
                                             as.matrix(treatedTest[,goodvars,drop=FALSE])))
  print(WVPlots::ROCPlot(treatedTrainP,mname,yName,
                         paste0('prediction on train, ntree=',ntrees)))
  print(WVPlots::ROCPlot(treatedTestP,mname,yName,
                         paste0('prediction on test, ntree=',ntrees)))
}




bDeviance <- function(y,pred) {
  -2*mean(ifelse(y,log(pred),log(1-pred)))
}


```


```{r shutdown}
if(!is.null(cl)) {
    parallel::stopCluster(cl)
    cl = NULL
}
```

